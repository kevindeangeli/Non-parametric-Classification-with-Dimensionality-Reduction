{\rtf1\ansi\ansicpg1252\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;\red191\green100\blue38;\red32\green32\blue32;\red153\green168\blue186;
\red109\green109\blue109;\red254\green187\blue91;\red152\green54\blue29;\red86\green132\blue173;\red88\green118\blue71;
\red117\green114\blue185;\red160\green0\blue163;\red128\green63\blue122;}
{\*\expandedcolortbl;;\csgenericrgb\c74902\c39216\c14902;\csgenericrgb\c12549\c12549\c12549;\csgenericrgb\c60000\c65882\c72941;
\csgenericrgb\c42745\c42745\c42745;\csgenericrgb\c99608\c73333\c35686;\csgenericrgb\c59608\c21176\c11373;\csgenericrgb\c33725\c51765\c67843;\csgenericrgb\c34510\c46275\c27843;
\csgenericrgb\c45882\c44706\c72549;\csgenericrgb\c62745\c0\c63922;\csgenericrgb\c50196\c24706\c47843;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0

\f0\fs34 \cf2 \cb3 import \cf4 numpy \cf2 as \cf4 np\
\cf2 import \cf4 pandas \cf2 as \cf4 pd\
\cf2 import \cf4 math \cf5 #Used for Pi and log()\
\cf2 import \cf4 sympy \cf2 as \cf4 sym\
\cf2 import \cf4 matplotlib.pyplot \cf2 as \cf4 plt\
\cf2 from \cf4 mpl_toolkits.mplot3d \cf2 import \cf4 Axes3D\
\cf2 from \cf4 numpy \cf2 import \cf4 ones\cf2 ,\cf4 vstack\
\cf2 import \cf4 time\
\
\cf2 def \cf6 load_data\cf4 (f):\
    \cf5 #f is the route of the data file.\
    \cf4 x = pd.read_csv(f\cf2 , \cf7 delim_whitespace\cf4 =\cf8 1\cf2 , \cf7 header\cf4 =\cf2 None\cf4 )\
    x[x.shape[\cf8 1\cf4 ] - \cf8 1\cf4 ] = x[x.shape[\cf8 1\cf4 ] - \cf8 1\cf4 ].map(\{\cf9 'Yes'\cf4 : \cf8 1\cf2 , \cf9 'No'\cf4 : \cf8 0\cf4 \})\
    \cf5 #Could return x if I wanted to keep it as the first project.\
    \cf4 X = x.loc[:\cf2 ,\cf8 0\cf4 :x.shape[\cf8 1\cf4 ]-\cf8 2\cf4 ]\
    Y = x.loc[:\cf2 ,\cf4 x.shape[\cf8 1\cf4 ]-\cf8 1\cf4 ]\
    Y=Y.to_numpy()\
    X=X.to_numpy()\
    X=normalization(X)\
    \cf2 return \cf4 X\cf2 , \cf4 Y\
\
\cf2 def \cf6 normalization\cf4 (X):\
    meanArr = np.mean(X\cf2 , \cf7 axis\cf4 =\cf8 0\cf4 )\
    varArr = np.std(X\cf2 , \cf7 axis\cf4 =\cf8 0\cf4 )\
    nX = X[:]\
    \cf2 for \cf4 i \cf2 in \cf10 range\cf4 (X.shape[\cf8 1\cf4 ]):\
        nX[:\cf2 , \cf4 i] = (X[:\cf2 , \cf4 i] - meanArr[i]) / varArr[i]\
    \cf2 return \cf4 nX\
\
\cf2 def \cf6 pca\cf4 (nX\cf2 ,\cf4 percentEerror=\cf8 .9\cf2 ,\cf4 showGraph=\cf2 False\cf4 ):\
    nX_Cov = np.cov(nX.T) \cf5 #Note that covariannce in pd is calculated differently\
    \cf4 nX_eig\cf2 , \cf4 nX_eigV = np.linalg.eig(nX_Cov)\
    ordered_eigs = -np.sort(-nX_eig)\
    totalSum = np.sum(ordered_eigs)\
    \cf5 # Store the indexes of the ordered eigenvalues\
    # So you can create a matrix of eigenvectors\
    # it the correct order:\
    \cf4 order_index_eigs = []\
    \cf2 for \cf4 i \cf2 in \cf10 range\cf4 (ordered_eigs.shape[\cf8 0\cf4 ]):\
        order_index_eigs.append(np.where(nX_eig == ordered_eigs[i])[\cf8 0\cf4 ].item())\
\
\
    \cf5 # Store the indexes of the ordered eigenvalues\
    # So you can create a matrix of eigenvectors\
    # it the correct order:\
    \cf4 order_index_ordered_eigs = []\
    \cf2 for \cf4 i \cf2 in \cf10 range\cf4 (ordered_eigs.shape[\cf8 0\cf4 ]):\
        order_index_ordered_eigs.append(np.where(nX_eig == ordered_eigs[i])[\cf8 0\cf4 ].item())\
\
\
    \cf2 def \cf6 eigenValErrorAnalysis\cf4 (ordered_eigs):\
        plt.figure(\cf7 num\cf4 =\cf2 None, \cf7 figsize\cf4 =(\cf8 8\cf2 , \cf8 8\cf4 )\cf2 , \cf7 dpi\cf4 =\cf8 100\cf2 , \cf7 facecolor\cf4 =\cf9 'w'\cf2 , \cf7 edgecolor\cf4 =\cf9 'k'\cf4 )\
        x = np.linspace(\cf8 1\cf2 , \cf4 ordered_eigs.shape[\cf8 0\cf4 ]\cf2 , \cf4 ordered_eigs.shape[\cf8 0\cf4 ])\
        \cf5 # Cumulative:\
        \cf4 eg = []\
        totalEig = \cf8 0\
        \cf2 for \cf4 i \cf2 in \cf10 range\cf4 (ordered_eigs.shape[\cf8 0\cf4 ]):\
            totalEig += ordered_eigs[i]\
            eg.append(totalEig / totalSum)\
        plt.plot(x\cf2 , \cf4 eg)\
        plt.xlabel(\cf7 xlabel\cf4 =\cf9 'Number of eigenvalues'\cf4 )\
        plt.ylabel(\cf7 ylabel\cf4 =\cf9 'Percent of Information Explained'\cf4 )\
        plt.show()\
\
    \cf2 if \cf4 showGraph == \cf2 True\cf4 :\
        eigenValErrorAnalysis(ordered_eigs)\
\
    \cf2 def \cf6 numberofEigValsToUse\cf4 (eigs\cf2 ,\cf4 percentEerror):\
        tmp = \cf8 0\
        \cf5 ix \cf4 = -\cf8 1\
        \cf4 P = []\
        \cf2 for \cf4 i \cf2 in \cf10 range\cf4 (eigs.shape[\cf8 0\cf4 ]):\
            tmp += eigs[i] / totalSum\
            \cf2 if \cf4 tmp > percentEerror:\
                ix = i + \cf8 1  \cf5 # Number of eigenvectors to use is 5\
                \cf2 for \cf4 k \cf2 in \cf10 range\cf4 (ix):\
                    a = nX_eigV[order_index_eigs[k]]\
                    P.append(a)\
                P = np.array(P)\
                \cf2 return \cf4 P\
\
    P = numberofEigValsToUse(ordered_eigs\cf2 ,\cf4 percentEerror)\
    \cf2 return \cf4 np.dot(nX\cf2 ,\cf4 np.transpose(P))\
\
\cf2 def \cf6 fld\cf4 (nX\cf2 ,\cf4 y=\cf8 0\cf2 , \cf4 training= \cf2 True\cf4 ):\
    \cf2 if \cf4 training:\
        \cf5 #split the data int two classes:\
        #key = y[:, 0] == 0\
        \cf4 key0 = y==\cf8 0\
        \cf4 y0Values = nX[key0]\
        key1 = y == \cf8 1\
        \cf4 y1Values= nX[key1]\
\
        y0ValuesMean = np.mean(y0Values\cf2 ,\cf7 axis\cf4 =\cf8 0\cf4 )\
        y1ValuesMean = np.mean(y1Values\cf2 ,\cf7 axis\cf4 =\cf8 0\cf4 )\
\
\
        y0Cov = np.cov(y0Values.T)\
        y1Cov = np.cov(y1Values.T)\
\
        S_0 = (y0Values.shape[\cf8 0\cf4 ] - \cf8 1\cf4 ) * y0Cov\
        S_1 = (y0Values.shape[\cf8 0\cf4 ] - \cf8 1\cf4 ) * y1Cov\
\
        S_w = S_0 + S_1\
        S_w_inv = np.linalg.inv(np.array(S_w))\
\
        fld.v = np.dot(S_w_inv\cf2 , \cf4 (np.transpose(y0ValuesMean) - np.transpose(y1ValuesMean)))\
\
\
        y0Values=np.dot(y0Values\cf2 , \cf4 fld.v)\
        y1Values=np.dot(y1Values\cf2 , \cf4 fld.v)\
\
\
        nX[:\cf2 , \cf8 0\cf4 ][key0] = y0Values\
        nX[:\cf2 , \cf8 0\cf4 ][key1] = y1Values\
        nX=np.delete(nX\cf2 , \cf4 np.s_[\cf8 1\cf4 :nX.shape[\cf8 1\cf4 ]]\cf2 , \cf7 axis\cf4 =\cf8 1\cf4 )\
    \cf2 else\cf4 :\
        \cf10 print\cf4 (fld.v)\
        nX = np.dot(nX\cf2 , \cf4 fld.v)\
    \cf2 return \cf4 nX\
\
\
\
\
\cf2 class \cf4 Knn:\
    \cf2 def \cf11 __init__\cf4 (\cf12 self\cf4 ):\
        \cf12 self\cf4 .nX = []\
        \cf12 self\cf4 .pX = []\
        \cf12 self\cf4 .fX = []\
        \cf12 self\cf4 .predictionArr =[]\
        \cf12 self\cf4 .totalTime= -\cf8 1\
\
    \cf2 def \cf6 showTime\cf4 (\cf12 self\cf4 ):\
        \cf10 print\cf4 (\cf9 "Time in seconds: "\cf2 , \cf12 self\cf4 .totalTime)\
\
    \cf2 def \cf6 fit\cf4 (\cf12 self\cf2 , \cf4 X\cf2 , \cf4 y):\
        \cf5 #self.nX = normalization(X)\
        \cf12 self\cf4 .nX = X\
        \cf12 self\cf4 .pX = pca(\cf12 self\cf4 .nX)\
        \cf12 self\cf4 .fX = fld(\cf12 self\cf4 .nX\cf2 ,\cf4 y)\
        \cf12 self\cf4 .y = y\
\
    \cf5 #x here is just a point\
    \cf2 def \cf6 euclidian_dsitanceList\cf4 (\cf12 self\cf2 , \cf4 x\cf2 , \cf4 X):\
        \cf5 # x is the test point, X is the dataset\
        # Using Euclidian Distance\
        \cf4 distancesArr = []\
        \cf5 # For each row in the data set:\
        dist \cf4 = -\cf8 1\
        \cf4 index=\cf8 0 \cf5 # used to associate a distance with a y.\
        \cf2 for \cf4 row \cf2 in \cf4 X:\
            testPoint = row[\cf8 0\cf4 :X.shape[\cf8 1\cf4 ]]\
            dist = np.linalg.norm(testPoint - x)\
            labelIndex = \cf12 self\cf4 .y[index]\
            distanceAndLabel = (dist\cf2 , \cf4 labelIndex)\
            distancesArr.append(distanceAndLabel)\
            index +=\cf8 1\
        \cf2 return \cf4 distancesArr\
\
\
    \cf5 #x here is just a point\
    \cf2 def \cf6 guessLabel\cf4 (\cf12 self\cf2 ,\cf4 x\cf2 , \cf4 X\cf2 , \cf4 k):\
        label0 = \cf8 0\
        \cf5 guessClass \cf4 = -\cf8 1\
        \cf4 label1 = \cf8 0\
        \cf4 ks = []\
        distancesArr = \cf12 self\cf4 .euclidian_dsitanceList(x\cf2 , \cf4 X)\
        distancesArr.sort()\
        \cf2 for \cf4 p \cf2 in \cf10 range\cf4 (\cf10 int\cf4 (k)):\
            minimum = \cf10 min\cf4 (distancesArr)\
            ks.append((minimum[\cf8 0\cf4 ]\cf2 , \cf4 minimum[\cf8 1\cf4 ]))\
            distancesArr.remove(minimum)\
        \cf2 for \cf4 q \cf2 in \cf10 range\cf4 (\cf10 len\cf4 (ks)):\
            \cf2 if \cf4 ks[q][\cf8 1\cf4 ] == \cf8 0\cf4 :\
                label0 += \cf8 1\
            \cf2 else\cf4 :\
                label1 += \cf8 1\
        \cf2 if \cf4 label0 >= label1:\
            guessClass = \cf8 0\
        \cf2 else\cf4 :\
            guessClass = \cf8 1\
        \cf2 return \cf4 guessClass\
\
\
    \cf2 def \cf6 knn\cf4 (\cf12 self\cf2 ,\cf4 XTest\cf2 , \cf4 X\cf2 , \cf4 k):\
        \cf5 #X is the training data\
        \cf4 guessList =[]\
        \cf5 guessedLabel \cf4 = -\cf8 1\
        \cf5 #print(XTest)\
        \cf2 for \cf4 row \cf2 in \cf4 XTest:\
            guessedLabel = \cf12 self\cf4 .guessLabel(row\cf2 , \cf4 X\cf2 , \cf4 k)\
            guessList.append(guessedLabel)\
        \cf2 return \cf4 guessList\
\
\
\
    \cf2 def \cf6 predict\cf4 (\cf12 self\cf2 , \cf4 XTest\cf2 , \cf4 k=\cf8 1\cf2 , \cf4 data=\cf9 "nX"\cf4 ):\
        start_time = time.time()\
        \cf5 predictionArr \cf4 =[]\
        \cf5 #print(XTest)\
        \cf2 if \cf4 data == \cf9 "nX"\cf4 :\
            \cf12 self\cf4 .predictionArr = \cf12 self\cf4 .knn(XTest\cf2 , \cf12 self\cf4 .nX\cf2 , \cf4 k)\
        \cf2 elif \cf4 data == \cf9 "pX"\cf4 :\
            XTest = pca(XTest)\
            \cf12 self\cf4 .predictionArr = \cf12 self\cf4 .knn(XTest\cf2 , \cf12 self\cf4 .pX\cf2 , \cf4 k)\
        \cf2 else\cf4 :\
            XTest=fld(XTest\cf2 , \cf7 training\cf4 =\cf2 False\cf4 )\
            \cf12 self\cf4 .predictionArr = \cf12 self\cf4 .knn(XTest\cf2 , \cf12 self\cf4 .fX\cf2 , \cf4 k)\
        \cf5 #print(self.predictionArr)\
        \cf12 self\cf4 .totalTime= time.time() - start_time\
        \cf2 return  \cf12 self\cf4 .predictionArr\
\
\
\cf2 def \cf6 accuracy_score\cf4 (yTest\cf2 , \cf4 y_model):\
    TP\cf2 , \cf4 TN\cf2 , \cf4 FP\cf2 , \cf4 FN = \cf8 0\cf2 , \cf8 0\cf2 , \cf8 0\cf2 , \cf8 0\
    \cf4 index=\cf8 0\
    \cf2 for \cf4 rightLabel \cf2 in \cf4 yTest:\
        guessLabel=y_model[index]\
        index+=\cf8 1\
        \cf2 if \cf4 guessLabel == rightLabel:\
            \cf2 if \cf4 rightLabel == \cf8 1\cf4 :\
                TP += \cf8 1\
            \cf2 else\cf4 :\
                TN += \cf8 1\
        \cf2 else\cf4 :\
            \cf2 if \cf4 rightLabel == \cf8 1\cf4 :\
                FN += \cf8 1\
            \cf2 else\cf4 :\
                FP += \cf8 1\
    \cf5 # print(TP)\
    \cf4 totalRowsInData = yTest.shape[\cf8 0\cf4 ]\
    confusion_matrix = [[\cf9 'TP'\cf2 , \cf4 TP]\cf2 , \cf4 [\cf9 "TN"\cf2 , \cf4 TN]\cf2 , \cf4 [\cf9 'FP'\cf2 , \cf4 FP]\cf2 , \cf4 [\cf9 'FN'\cf2 , \cf4 FN]\cf2 , \cf4 [\cf9 'Accuracy'\cf2 , \cf4 (TN + TP) / totalRowsInData]]\
    confusionarr = [TP\cf2 , \cf4 TN\cf2 , \cf4 FP\cf2 , \cf4 FN]\
    \cf10 print\cf4 (confusion_matrix)\
    \cf2 return \cf4 confusion_matrix\cf2 , \cf4 confusionarr\
\
\
\
\
\
\
\
\
\
\
\cf2 def \cf6 main\cf4 ():\
    trainingData=  \cf9 '/Users/kevindeangeli/Desktop/Fall2019/COSC522/Project2/Dataset_PimaTr.txt'\
    \cf4 testingData= \cf9 '/Users/kevindeangeli/Desktop/Fall2019/COSC522/Project2/Dataset_PimaTest.txt'\
    \cf4 Xtrain\cf2 ,\cf4 Ytrain = load_data(trainingData)\
    Xtest\cf2 ,\cf4 ytest = load_data(testingData)\
    model = Knn()\
    model.fit(Xtrain\cf2 , \cf4 Ytrain)\
\
    \cf5 #Predicts accepts values "nX", "pX", "fX". "nX" is by default.\
    \cf4 y_model = model.predict(Xtest\cf2 ,\cf7 k\cf4 =\cf8 21\cf2 , \cf7 data\cf4 =\cf9 "fX"\cf4 )\
    \cf5 accuracy \cf4 = accuracy_score(ytest\cf2 , \cf4 y_model)\
\
\
    \cf5 #print('accuracy = ', accuracy)\
\
\
\cf2 if \cf4 __name__ == \cf9 "__main__"\cf4 :\
    main()\
}